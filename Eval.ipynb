{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import LxmertTokenizer, LxmertForPreTraining, AdamW\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# NLVR2 Dataset\n",
    "class NLVR2Dataset(Dataset):\n",
    "    def __init__(self, json_file, img_dir, tokenizer, transform=None):\n",
    "        with open(json_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item['sentence']\n",
    "        label = int(item['label'])\n",
    "        img_left = Image.open(os.path.join(self.img_dir, item['identifier'] + '-img0.png')).convert('RGB')\n",
    "        img_right = Image.open(os.path.join(self.img_dir, item['identifier'] + '-img1.png')).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img_left = self.transform(img_left)\n",
    "            img_right = self.transform(img_right)\n",
    "\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'img_left': img_left,\n",
    "            'img_right': img_right,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Model definition\n",
    "class LXMERTFORNLVR2(torch.nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super().__init__()\n",
    "        self.lxmert = LxmertForPreTraining.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "        self.classifier = torch.nn.Linear(self.lxmert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, visual_feats, visual_pos):\n",
    "        outputs = self.lxmert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            visual_feats=visual_feats,\n",
    "            visual_pos=visual_pos,\n",
    "        )\n",
    "        pooled_output = outputs.pooled_output\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        img_left = batch['img_left'].to(device)\n",
    "        img_right = batch['img_right'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        visual_feats = torch.cat([img_left, img_right], dim=1)\n",
    "        visual_pos = torch.arange(2 * 196).repeat(input_ids.size(0), 1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, visual_feats, visual_pos)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            img_left = batch['img_left'].to(device)\n",
    "            img_right = batch['img_right'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            visual_feats = torch.cat([img_left, img_right], dim=1)\n",
    "            visual_pos = torch.arange(2 * 196).repeat(input_ids.size(0), 1).to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, visual_feats, visual_pos)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary')\n",
    "    return accuracy, f1\n",
    "\n",
    "# Main training loop\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    batch_size = 32\n",
    "    num_epochs = 5\n",
    "    learning_rate = 2e-5\n",
    "\n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Tokenizer and model\n",
    "    tokenizer = LxmertTokenizer.from_pretrained(\"unc-nlp/lxmert-base-uncased\")\n",
    "    model = LXMERTFORNLVR2().to(device)\n",
    "\n",
    "    # Data preparation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    train_dataset = NLVR2Dataset('path/to/train.json', 'path/to/train/images', tokenizer, transform)\n",
    "    val_dataset = NLVR2Dataset('path/to/val.json', 'path/to/val/images', tokenizer, transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, device)\n",
    "        val_accuracy, val_f1 = evaluate(model, val_loader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'lxmert_nlvr2_model.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
